# -*- coding: utf-8 -*-
"""Итоговая Python/Spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5QTLcDCz0cqWeVq_1BtZT0aVSXm1whb

**PYTHON**
"""

import pandas as pd

"""**1. Загрузить в колаб файлы по оценкам (ratings) и фильмам (movies) и создать на их основе pandas-датафреймы**

"""

df_ratings = pd.read_csv('u.data.csv', delimiter='\t', names=['user_id',	'item_id', 'rating', 'timestamp'] )
df_ratings.head()

df_ratings.describe()

df_movies = pd.read_csv('u.item.csv', delimiter='|', names=["Movie Id", "Movie Title", "Release Date", "Video Release Date", "IMDb URL", "unknown", "Action", "Adventure", "Animation",	"Children's",	"Comedy", "Crime", "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western" ], encoding='ISO-8859–1' )

df_movies.head()

df_movies.describe()

"""**2. Используя dataframe ratings, найти id пользователя, поставившего больше всего оценок**"""

#Оставляю только те солбцы, которые нужны для рассчета
df_count = df_ratings[['user_id', 'rating']]
df_count

#Группирую по user_id, считаю количество значений. После агрегации user_id стал индексом, поэтому для получения индекса с максимальным кол-вом оценок использую idxmax
df_count.groupby('user_id').count()['rating'].idxmax()

"""**3. Оставить в датафрейме ratings только те фильмы, который оценил данный пользователь**"""

df_user = df_ratings[df_ratings['user_id'] == 405]
df_user

"""Для построения модели нам нужны признаки. В качестве таковых будем использовать:
* Год выхода
* Жанры
* Общее количество оценок
* Суммарную оценку

**4.** **Добавить к датафрейму из задания 3 столбцы:**
* **По жанрам. Каждый столбец - это жанр. Единицу записываем, если фильм   принадлежит данному жанру и 0 - если нет.**
* **Столбцы с общим количеством оценок от всех пользователей на фильм и суммарной оценкой от всех пользователей**
"""

df_movies.columns

# Соединяю датафреймы df_user и df_movies, из второго беру только необходимые столбцы
df_merdged = df_user.merge(df_movies[['Movie Id', 'Release Date', 'unknown', 'Action', 'Adventure', 'Animation', "Children's",
       'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir',
       'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War',
       'Western' ]], left_on= 'item_id', right_on= 'Movie Id', how= 'inner')
df_merdged

# Создаю новый датафрейм, где считаю кол-во оценок по каждому фильму из датафрейма retings
df_item_count = df_ratings[['item_id', 'rating']].groupby('item_id').count()
df_item_count

#Для удобства восприятия переименую столбец rating в rating_quantity
df_item_count.rename(columns={'rating':'rating_quantity'}, inplace=True)

#Добавляю столбец, в котором вычисляю сумму оценок
df_item_count['rating_amount'] = df_ratings[['item_id', 'rating']].groupby('item_id').sum()
df_item_count

# Соединяю с основным датафреймом
df_merdged = df_merdged.merge(df_item_count, left_on= 'item_id', right_on= 'item_id', how= 'inner')
df_merdged

df_merdged.describe()

"""* **Построить простую модель для рекомендации фильмов пользователям**

"""

df_merdged.dtypes

# Так как для построения модели нам нужен параметр "Год выхода",  преобразовываю данные в столбце 'Release Date' из типа данных object в date 
df_merdged['Release Date'] = pd.to_datetime(df_merdged['Release Date'])

# Добавляю столбец, в котором будет только год выпуска
df_merdged['Release Year'] = df_merdged['Release Date'].dt.year
df_merdged

"""**5. Сформировать X_train, X_test, y_train, y_test**"""

# Разделяю данные на атрибуты и метки
df_merdged.columns

X,y = df_merdged[['unknown', 'Action', 'Adventure', 'Animation', "Children's", 'Comedy',
       'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
       'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western',
       'rating_quantity', 'rating_amount', 'Release Year']], df_merdged['rating']

# Разбиваю данные на данные для обучения и проверки
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""**6. Обучить модель линейной регрессии (или любую другую для задачи регрессии)  на фильмах**"""

from sklearn.linear_model import LinearRegression

# Создаю и обучаю модель LinearRegression
Lr_model = LinearRegression().fit(X_train, y_train)

# Делаю прогноз на тестовых данных и визуально сравниваю фактические значения с прогнозом
df_predict = pd.DataFrame({'raiting': y_test, 'predicted': Lr_model.predict(X_test)}) 
df_predict

"""**7. Оценить качество модели на X_test, y_test при помощи метрик для задачи регрессии**"""

from sklearn.metrics import mean_absolute_percentage_error

mean_absolute_percentage_error(y_test, Lr_model.predict(X_test))

mean_absolute_percentage_error(y_train, Lr_model.predict(X_train))

"""**SPARK**"""

!apt-get update

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://downloads.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop2.7.tgz

!tar -xvf spark-3.2.2-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.2-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").getOrCreate()

"""**8. Загрузить данные в spark**"""

# Загружаю данные из item, задаю названия колонкам и сразу удаляю те, которые не нужны для дальнейших задач
df_item = spark.read.csv('u.item.csv', inferSchema=True, sep= '|')\
  .toDF("Movie_id", "Movie_Title", "Release Date", "Video Release Date", "IMDb URL", "unknown", "Action", "Adventure", "Animation",	"Children",	"Comedy", "Crime", "Documentary", "Drama", "Fantasy", "Film_Noir", "Horror", "Musical", "Mystery", "Romance", "Sci_Fi", "Thriller", "War", "Western" )\
  .drop("Movie_Title", "Release Date", "Video Release Date", "IMDb URL" )
df_item.show()

# Загружаю данные из data, выполняю все действия по аналогии с предыдущим датафреймом
df_data = spark.read.csv('u.data.csv', inferSchema=True, sep= '\t').toDF('user_id',	'item_id', 'rating', 'timestamp').select('item_id', 'rating')
df_data.show()

"""**9. Вывести среднюю оценку для каждого фильма**"""

df_avg = df_data.groupBy('item_id').avg('rating').orderBy('item_id')
df_avg.show()

"""**10. Посчитать среднюю оценку для каждого жанра**"""

for i in range(len(df_item.columns)):
    if i <= 1:
        continue  
    if i == len(df_item.columns):
        break
    df_genre = df_item.filter(df_item[i] == 1).join(df_data, df_item.Movie_id == df_data.item_id, 'inner').agg({df_data.columns[1]:'avg'})
    df_genre = df_genre.withColumnRenamed(df_genre.columns[0], df_item.columns[i])
    df_genre.show()

"""**11. В спарке получить 2 датафрейма с 5-ю самыми популярными и самыми непопулярными фильмами (по количеству оценок, либо по самой оценке - на Ваш выбор)**"""

from pyspark.sql.functions import desc

# Оцениваю популярность по средней оценке. Получаю 5 самых популярных фильмов
df_top = df_avg.orderBy(desc('avg(rating)')).show(5)

# Нахожу 5 самых непопулярных фильмов 
df_luz = df_avg.orderBy('avg(rating)').show(5)